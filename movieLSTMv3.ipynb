{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import string\n",
    "\n",
    "class MovieScriptDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer):\n",
    "        self.input_ids = tokenizer.encode(text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.input_ids[idx]}\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    translation_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    cleaned_text = input_text.translate(translation_table)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_whitespace(input_text):\n",
    "    lines = input_text.split('\\n')\n",
    "    non_empty_lines = [line for line in lines if line.strip()]\n",
    "    replaced_text = '\\n'.join(non_empty_lines).replace('\\t', ' ')\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', replaced_text)\n",
    "    return cleaned_text.strip() \n",
    "\n",
    "def read_movie_script(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        movie_script = file.read()\n",
    "    return remove_punctuation(remove_whitespace(movie_script.lower()))\n",
    "\n",
    "def fine_tune_model(model, tokenizer, train_dataset, num_train_epochs=3, batch_size=2):\n",
    "    model.train()\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    model.save_pretrained(\"./gpt2-finetuned\")\n",
    "\n",
    "movie_script = read_movie_script(\"samplerv1.txt\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "training_dataset = MovieScriptDataset(movie_script, tokenizer)\n",
    "fine_tune_model(model, tokenizer, training_dataset, 20, 64)\n",
    "\n",
    "prompt_text = \"Suddenly it all went black\"\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=1000, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
